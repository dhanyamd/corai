from maya.chains import get_chain, get_code_response, get_instructions
from maya.state import CoraiAgentState
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AIMessage, HumanMessage, RemoveMessage
import re
from e2b_code_interpreter import Sandbox
from openevals.code.e2b.pyright import create_async_e2b_pyright_evaluator
from langsmith import traceable
from maya.settings import Settings

@traceable
async def input_node(state: CoraiAgentState, config: RunnableConfig ): 
    """
    Contains a prompt and code from the user as input, then generates the detailed
    summary for the prompt and returns the detailed summary and code back.

    Args: 
        messages: str | list[str] | list[dict[str, any]]
    Yields: 
        summary: str 
        code : str | list[str]
    """
    initial_prompt = state.get("messages", "")
    print(f"Input Node received: {initial_prompt}")
    chain = get_chain()
    response = await chain.ainvoke({"initial_prompt": initial_prompt, "messages": initial_prompt}, config)
    # Extract the summary from the response
    if hasattr(response, 'content'):
        summary = response.content
    else:
        summary = str(response)
    return {"summary": summary}
    
@traceable
async def response_node(state: CoraiAgentState, config: RunnableConfig):
    """
    Takes the summary generated and code back by the input_node back and generates code
    for the requested query. The response would be the code block generated by the LLM.
    
    Args:
      summary (str) : The summary generated by the input_node
    Returns:
      code (str): The response or the code generated through the summary and the code provided.
      It returns a code block containing all the changes as guided by the summary prompt.
    """
    summary = state.get("summary", "")
    sandbox_err = state.get("sandbox_response_err", "")
    print(f"sandbox error", sandbox_err)
    print(f"summary: ", {summary})
    # Extract the actual summary content if it's a dictionary or other object
    if isinstance(summary, dict):
        # If it's a dictionary, extract the content
        summary_content = str(summary)
    elif hasattr(summary, 'content'):
        # If it's an object with a content attribute, use that
        summary_content = summary.content
    else:
        # Otherwise, convert to string
        summary_content = str(summary)
    
    # Include sandbox error in the summary if it exists
    if sandbox_err:
        # Extract the error content if it's a dictionary
        if isinstance(sandbox_err, dict) and "error" in sandbox_err:
            error_content = "\n".join(sandbox_err["error"])
        else:
            error_content = str(sandbox_err)
        
        # Add the error information to the summary
        summary_content = f"{summary_content}\n\nPrevious sandbox execution error:\n{error_content}"
    
    # Always generate the code, but do not prepend instructions
    escaped_summary = summary_content.replace("{", "{{").replace("}", "}}") if summary_content else ""
    code_chain = get_code_response(summary=escaped_summary)
    code_gen = await code_chain.ainvoke({"summary": escaped_summary, "messages": state["messages"]}, config)
    
    # Extract content from the code generation
    code = code_gen.content if hasattr(code_gen, 'content') else str(code_gen)
    
    # Add the summary content as a new message to the messages list
    messages = state["messages"] + [HumanMessage(content=summary_content)]
    return {"code": AIMessage(content=code), "messages": messages}

@traceable
def sandbox_node(state: CoraiAgentState):
    """
    Responsible for running the code in a sandbox background and returning the local result.

    Args:
     code (str): The code generated by the response_node
    Returns:
     locals (dict | list[dict[str, any]]): The metadata response given by the sandbox agent after running the code.
    """
    code = state.get("code")

    if not code:
        # Handle the case where no code is provided
        state["sandbox_response"] = ["No code found in the state to run."]
        return state

    # Create a new sandbox instance
    settings = Settings()
    with Sandbox(api_key=settings.E2B_API_KEY) as sandbox:
        # Extract content from AIMessage object if needed
        if hasattr(code, 'content'):
            code_content = code.content
        else:
            code_content = str(code)
            
        # Extract code from markdown block if present
        code_match = re.search(r"```(?:python\n)?(.*)```", code_content, re.DOTALL)
        if code_match:
            code_content = code_match.group(1).strip()

        # The complex logic for separating setup commands and code is error-prone.
        # We will treat the entire content as a self-contained script.
        actual_code_to_run = code_content
        
        # Get the original code snippet from the messages
        original_code_snippet = ""
        if len(state["messages"]) > 1:
            # Assuming the second message is the code snippet
            if hasattr(state["messages"][1], 'content'):
                original_code_snippet = state["messages"][1].content

        # Determine a suitable filename based on the project type
        is_python_project_content = (
            'def ' in actual_code_to_run and
            ('import ' in actual_code_to_run or 'from ' in actual_code_to_run or 'unittest' in actual_code_to_run or 'pytest' in actual_code_to_run)
        )

        if is_python_project_content:
            filename = 'test_script.py'
        else:
            filename = 'script.js'
            
        # Determine the test runner
        test_runner = 'unittest'

        # Combine all setup and execution commands into a single command string
        import base64
        
        # Update the import statement in the test code
        actual_code_to_run = actual_code_to_run.replace("from your_module", "from module_to_test")
        
        # Encode the test code
        encoded_test_code = base64.b64encode(actual_code_to_run.encode('utf-8')).decode('utf-8')
        write_test_code_cmd = f"python -c \"import base64; decoded = base64.b64decode('{encoded_test_code}'); open('{filename}', 'wb').write(decoded)\""
        
        # Start with an empty script
        full_script = ""
        
        # If we have the original code, write it to a separate file
        if original_code_snippet:
            encoded_original_code = base64.b64encode(original_code_snippet.encode('utf-8')).decode('utf-8')
            write_original_code_cmd = f"python -c \"import base64; decoded = base64.b64decode('{encoded_original_code}'); open('module_to_test.py', 'wb').write(decoded)\""
            full_script += f"{write_original_code_cmd} && "
            
        # Add the command to write the test file
        full_script += write_test_code_cmd
        
        # Add setup and execution commands based on the test runner
        if test_runner == 'unittest':
            full_script += f" && python -m unittest {filename}"
        elif test_runner == 'pytest':
            full_script += f" && pip install pytest && python -m pytest"
        
        # Debug output
        print(f"Debug - filename: {filename}")
        print(f"Debug - full_script: {full_script}")
        
        # Run the combined script using the generic `run_command`
        try:
            proc = sandbox.commands.run(cmd=full_script, timeout=30000)  # 30 second timeout
            output = proc.stdout.split('\n')
            if proc.stderr:
                output.extend(["--- STDERR ---", *proc.stderr.split('\n')])

            if proc.exit_code == 0:
                # Success: Tests passed
                state["sandbox_response"] = {"output": output, "code": actual_code_to_run}
                if "sandbox_response_err" in state:
                    del state["sandbox_response_err"]
            elif proc.exit_code == 1:
                # Retryable error: Test failure
                state["sandbox_response_err"] = {"error": output, "code": actual_code_to_run}
            else:
                # Non-retryable error: Other sandbox error
                state["sandbox_response"] = {"output": output, "code": actual_code_to_run}
                if "sandbox_response_err" in state:
                    del state["sandbox_response_err"]

        except Exception as e:
            # Retryable error: Sandbox execution exception (e.g., timeout)
            state["sandbox_response_err"] = {
                "error": [f"Exception occurred: {str(e)}"],
                "code": actual_code_to_run,
            }
        return state

@traceable
def final_response(state: CoraiAgentState):
    """
    Returns the final code output to the user that is ran through the sandbox

    Args:
    sandbox_response (dict): the response for test cases generated by the sandbox
    final_correct_code (str | list[str]): the final correct code that ran through succesfully in the sandbox

    Returns:
    final_response (str | list[str]) : the final code returned and stored in state by combining the two arguments.
    """
    sandbox_response = state.get("sandbox_response", "")
    final_correct_code = state.get("code", "")
    
    # Handle the new format where sandbox_response contains both output and code
    if isinstance(sandbox_response, dict) and "output" in sandbox_response and "code" in sandbox_response:
        # New format: sandbox_response is a dict with "output" and "code" keys
        output = sandbox_response["output"]
        code = sandbox_response["code"]
        # Create a descriptive final response indicating this is the best case
        final_response = {
            "status": "tested efficiently",
            "description": "This is the best case among the following ran examples!",
            "output": output,
            "code": code
        }
    else:
        # Old format or error case
        # Handle both list and string cases for sandbox_response
        if isinstance(sandbox_response, list):
            final_response = {
                "status": "tested efficiently",
                "description": "This is the best case among the following ran examples!",
                "output": sandbox_response,
                "code": final_correct_code
            }
        else:
            final_response = {
                "status": "tested efficiently",
                "description": "This is the best case among the following ran examples!",
                "output": [sandbox_response],
                "code": final_correct_code
            }
            
    state["final_response"] = final_response
    return {"final_response": final_response, "messages": state["messages"]}
